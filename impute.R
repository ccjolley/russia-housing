library(mice)
library(ggplot2)

source('clean.R')
train <- read.csv('train.csv') %>% russia_clean

# Just running MICE on the whole thing is too slow -- I need something more
# efficient. Which variables are actually missing the most often?

nmis <- train %>% is.na %>% colSums %>% sort(decreasing=TRUE)
nmis <- nmis[nmis > 0]
length(nmis) # 53 variables have missing values
qplot(1:length(nmis),nmis/nrow(train))

###############################################################################
# Categories of variables in the Russia housing dataset
###############################################################################
nt <- names(train)
dont_use <- c('id','timestamp','ts','price_doc','ln_price_doc','ecology')
building <- c('build_year','state','kitch_sq','max_floor','material',
              'num_room','ln_life_sq','floor','ln_full_sq','product_type')
loc <- c(grep('_count_',nt,value=TRUE),
         grep('_km',nt,value=TRUE),
         grep('_part',nt,value=TRUE),
         grep('_sqm_',nt,value=TRUE),
         grep('raion',nt,value=TRUE),
         grep('male',nt,value=TRUE),
         grep('female',nt,value=TRUE),
         grep('_all',nt,value=TRUE),
         grep('_walk',nt,value=TRUE),
         grep('_avto',nt,value=TRUE),
         grep('_price_',nt,value=TRUE),
         grep('school',nt,value=TRUE),
         grep('_1line',nt,value=TRUE),
         'eco_scale','area_m','culture_objects_top_25')
factors <- c(grep('^ID_',nt,value=TRUE),'sub_area')
nt %>% setdiff(c(dont_use,building,loc,factors))



###############################################################################
# Given a cleaned dataset, perform PCA on the location-based variables with
# no missing values. Return a dataframe containing the non-complete variables
# along with the first nvars principal components of the complete ones.
###############################################################################
clean_to_pca1 <- function(df,nvars=30,never_use=NULL) {
  nmis <- df %>% is.na %>% colSums
  missing <- names(df)[nmis > 0]
  use_vars <- names(df) %>% setdiff(missing) %>% setdiff(never_use)
  pr <- prcomp(df[,use_vars],center=TRUE,scale=TRUE)
  diag <- data.frame(sd=pr$sdev) %>%
    mutate(var=sd^2,
           prop=var/sum(var),
           cum_prop=cumsum(prop))
  paste0('First ',nvars,' components contain ',diag[nvars,'cum_prop'],
         ' of variance') %>% print
  df[,!(names(df) %in% use_vars)] %>%
    cbind(pr$x[,1:nvars])
}



###############################################################################
# Given output from clean_to_pca1, generate an imputation matrix that can
# be used by MICE. I want to make something general enough that it will be 
# useful for other projects after this, so I'll have it take the following:
# df - the dataframe that requires imputation, with the rightmost columns
#      named PC1, PC2, etc.
# full_dep - an array of variables that depend on the principal componennts, or 
#            NULL if we're making all incomplete variables depend on them.
# groups - this is a list of arrays, each containing a group of interdependent
#          variables that should depend on each other during imputation. In
#          this case, these are my building-related variables.
###############################################################################
make_imp_matrix <- function() {
  
  
}
  
###############################################################################
# Given a dataframe and a factor variable, expand it into binary dummy 
# variables. Returns the dataframe without the original factor but with
# the dummy variables added.
###############################################################################
add_dummy <- function(df,varname) {}

###############################################################################
# Given output from clean_to_pca1(), impute missing values. This will also
# take a matrix generated by make_imp_matrix()
###############################################################################
pca1_to_imputed <- function()
  
###############################################################################
# Given a fully-imputed dataframe with no missing values, separate variables
# into pre-specified groups, run PCA on each, and output a dataframe where
# the specified number of PC's from each category has been kept.
###############################################################################
group_pca <- function()
# The thing to do here is just to start with PCA for complete variables and use that for
# imputation

loc_complete <- setdiff(names(train),names(nmis)) %>% 
  setdiff(c('price_doc','ln_price_doc','ts','id','timestamp','product_type','ecology'))
# dummy variables didn't help much
train_loc <- train[,loc_complete] %>%
  select(-sub_area,-ID_metro,-ID_railroad_station_avto,-ID_big_road1,
         -ID_big_road2,-ID_railroad_terminal,-ID_bus_terminal)
# Now it's PCA time
pr <- prcomp(train_loc,center=TRUE,scale=TRUE)
qplot(pr$x[,1],pr$x[,2])
data.frame(sd=pr$sdev) %>%
  mutate(var=sd^2,
         prop=var/sum(var),
         cum_prop=cumsum(prop),
         pc=row_number()) %>%
  ggplot(aes(x=pc,y=cum_prop)) +
    geom_point(color='cornflowerblue') +
    theme_classic() +
    geom_hline(yintercept=0.9,color='tomato')
# about 30 vars gets me to 90% of the location-related variance

# These only depend on location; impute them based on my 30 PCA variables
# There are 43 of them; I could expect this part to take ~20 min.
loc_dep <- c('preschool_quota','school_quota','cafe_sum_1000_min_price_avg',
             'cafe_sum_1000_max_price_avg','cafe_avg_price_1000',
             'raion_build_count_with_material_info','build_count_block',
             'build_count_wood','build_count_frame','build_count_brick',
             'build_count_monolith','build_count_panel','build_count_foam',
             'build_count_slag','build_count_mix',
             'raion_build_count_with_builddate_info','build_count_before_1920',
             'build_count_1921.1945','build_count_1946.1970',
             'build_count_1971.1995','build_count_after_1995',
             'cafe_sum_1500_min_price_avg','cafe_sum_1500_max_price_avg',
             'cafe_avg_price_1500','cafe_sum_2000_min_price_avg',
             'cafe_sum_2000_max_price_avg','cafe_avg_price_2000',
             'cafe_sum_3000_min_price_avg','cafe_sum_3000_max_price_avg',
             'cafe_avg_price_3000','cafe_sum_5000_min_price_avg',
             'cafe_sum_5000_max_price_avg','cafe_avg_price_5000',
             'prom_part_5000','metro_min_walk','metro_km_walk',
             'railroad_station_walk_km','railroad_station_walk_min',
             'ID_railroad_station_walk','hospital_beds_raion',
             'cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg',
             'cafe_avg_price_500')

# These concern specific buildings -- they'll be informed by location, but 
# also depend on each other.


# don't impute target variables (price), irrelevant variables (timestamp),
# or variables that have been replaced (ecology)
# also don't impute factor variables with lots of levels (ID_*)
impute_me <- train[,!(names(train) %in% loc_complete)] %>%
  select(-id,-timestamp,-price_doc,-ln_price_doc,-ts,-ecology,
         -ID_railroad_station_walk) %>%
  cbind(pr$x[,1:30])
               
nim <- names(impute_me)
m <- matrix(0,nrow=length(nim),ncol=length(nim))
for (ni in nim) {
  i <- which(nim==ni)
  # location for everything
  if (ni %in% names(nmis)) {
    m[i,(length(nim)-29):length(nim)] <- 1
  }
  # building also depend on each other
  if (ni %in% building) {
    other_building <- which(nim %in% building & nim != ni)
    m[i,other_building] <- 1
  }
}

# merge imputed variables back into train

imputed <- mice(impute_me,m=1,predictorMatrix=m)

saveRDS(imputed, file="imputed.rds")
#imputed <- readRDS("imputed.rds")

ci <- complete(imputed)
train_imp <- train 
for (n in nim[1:(length(nim)-30)]) {
  train_imp[,n] <- ci[,n]
}
# fix factor variables with few levels

is.na(train_imp) %>% colSums 
# only ID_railroad_station_walk still has NA's, but I'm not going to use
# those ID variables in PCA anyway.

# separate building and location-related variables, do PCA separately

nti <- names(train_imp)
dont_use <- c('id','timestamp','ts','price_doc','ln_price_doc','ecology')
factors <- c('sub_area',grep('^ID',nti,value=TRUE))
build_all <- c(building,'product_type')
loc_all <- c('eco_scale',loc_dep,loc_complete) %>% setdiff(factors)
setdiff(nti,c(dont_use,factors,build_all,loc_all))

# TODO: why wasn't eco_scale in loc_dep?

pc_loc <- prcomp(train_imp[,loc_all],center=TRUE,scale=TRUE)
# I now need 38 out of 271 components to get 90% of variance
pc_loc_x <- pc_loc$x[,1:38]
colnames(pc_loc_x) <- sub('PC','LOC',colnames(pc_loc_x))

buildvars <- train_imp[,build_all] %>%
  mutate(product_type=as.numeric(product_type)) %>%
  cbind(model.matrix(~train_imp$material)) %>% 
  select(-material,-`(Intercept)`)
pc_build <- prcomp(buildvars,center=TRUE,scale=TRUE)
# I can get 90% of variance with 9 of these 14 variables
pc_build_x <- pc_build$x[,1:9]
colnames(pc_build_x) <- sub('PC','B',colnames(pc_build_x))

# combine back into one data frame with PCA results and ln_price_doc

train_pc <- train %>% 
  select(ln_price_doc) %>%
  cbind(pc_build_x,pc_loc_x)

# how well do linear models do now?

lm_pca <- lm(ln_price_doc ~ .,train_pc)
pred_pca <- train_pc %>% select(ln_price_doc)
pred_pca <- merge(pred_pca,predict(lm_pca),by='row.names')

(pred_pca$ln_price_doc - pred_pca$y)^2 %>% mean %>% sqrt
# 0.4888297, compared to 0.5151 in linear.R
qplot(pred_pca$ln_price_doc,pred_pca$y)

# does test set have any missing values in it?

test_raw <- read.csv('test.csv') # this does contain some NA's
test <- test_raw %>% russia_clean

# TODO: I need a function that, given an original data row with possible 
# missing values (as in the test set), will do the following:
# 1. initial location-based PCA projection (based on data from training set)

test_pc1 <- test[,loc_complete] %>%
  select(-sub_area,-ID_metro,-ID_railroad_station_avto,-ID_big_road1,
         -ID_big_road2,-ID_railroad_terminal,-ID_bus_terminal) %>%
  predict(pr,.)
test_pc1 <- test_pc1[,1:30]

# 2. imputation based on data from training data

test_impute_me <- test[,!(names(test) %in% loc_complete)] %>%
  select(-id,-timestamp,-price_doc,-ln_price_doc,-ts,-ecology,
         -ID_railroad_station_walk) %>%
  cbind(test_pc1)

# I think the right way to do this will be to rbind the training
# and test datasets together so that the training set informs
# the imputations in the test set as well as possible.

# I'm starting to wonder if I should take that approach for PCA
# as well -- seems like the principled way to do out-of-sample
# evaluation.

# 3. PCA projection using imputed values
# 4. return a row with just the needed PCA components


# then I can deploy this on a linear model (or whatever else I fancy)
